{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FugueSQL in 10 Minutes\n",
    "\n",
    "All questions are welcome in the Slack channel.\n",
    "\n",
    "[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://slack.fugue.ai)\n",
    "\n",
    "This is a short introduction of FugueSQL geared for new users. FugueSQL is the SQL interface for [Fugue](https://github.com/fugue-project/fugue). The Fugue project aims to make big data effortless by accelerating iteration speed and providing a simpler interface for users to utilize distributed computing engines.\n",
    "\n",
    "This tutorial only covers the SQL interface. For Python, check the [Fugue in 10 minutes section](ten_minutes.ipynb). Note that this is just an overview of the features, not a full tutorial.\n",
    "\n",
    "FugueSQL is meant for SQL lovers who want to use their preferred grammar of choice on top of Pandas, Spark and Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "There are two things to install. First is FugueSQL (which is separate from Fugue). Install it with:\n",
    "\n",
    "```\n",
    "pip install fugue[sql]\n",
    "```\n",
    "\n",
    "FugueSQL has a notebook extension for both Jupyter Notebooks and JupyterLab. This extension provides syntax highlughting and To install the extension, use pip:\n",
    "\n",
    "```\n",
    "pip install fugue-jupyter\n",
    "```\n",
    "\n",
    "and then to register the startup script:\n",
    "\n",
    "```\n",
    "fugue-jupyter install startup\n",
    "```\n",
    "\n",
    "See [this documentation](https://github.com/fugue-project/fugue-jupyter) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you are using Jupyter lab and followed the installation instructions above, then the `%%fsql` cell magic is already registered by default. Otherwise, it can be used using the following command where `is_lab` indicates if you are using Jupyter Lab versus Classic Jupyter Notebooks. This `setup()` gives both the cell magic and the syntax highlighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\nrequire([\"codemirror/lib/codemirror\"]);\nfunction set(str) {\n    var obj = {}, words = str.split(\" \");\n    for (var i = 0; i < words.length; ++i) obj[words[i]] = true;\n    return obj;\n  }\nvar fugue_keywords = \"fill hash rand even presort persist broadcast params process output outtransform rowcount concurrency prepartition zip print title save append parquet csv json single checkpoint weak strong deterministic yield connect sample seed take sub callback dataframe file\";\nCodeMirror.defineMIME(\"text/x-fsql\", {\n    name: \"sql\",\n    keywords: set(fugue_keywords + \" add after all alter analyze and anti archive array as asc at between bucket buckets by cache cascade case cast change clear cluster clustered codegen collection column columns comment commit compact compactions compute concatenate cost create cross cube current current_date current_timestamp database databases data dbproperties defined delete delimited deny desc describe dfs directories distinct distribute drop else end escaped except exchange exists explain export extended external false fields fileformat first following for format formatted from full function functions global grant group grouping having if ignore import in index indexes inner inpath inputformat insert intersect interval into is items join keys last lateral lazy left like limit lines list load local location lock locks logical macro map minus msck natural no not null nulls of on optimize option options or order out outer outputformat over overwrite partition partitioned partitions percent preceding principals purge range recordreader recordwriter recover reduce refresh regexp rename repair replace reset restrict revoke right rlike role roles rollback rollup row rows schema schemas select semi separated serde serdeproperties set sets show skewed sort sorted start statistics stored stratify struct table tables tablesample tblproperties temp temporary terminated then to touch transaction transactions transform true truncate unarchive unbounded uncache union unlock unset use using values view when where window with\"),\n    builtin: set(\"date datetime tinyint smallint int bigint boolean float double string binary timestamp decimal array map struct uniontype delimited serde sequencefile textfile rcfile inputformat outputformat\"),\n    atoms: set(\"false true null\"),\n    operatorChars: /^[*\\/+\\-%<>!=~&|^]/,\n    dateSQL: set(\"time\"),\n    support: set(\"ODBCdotTable doubleQuote zerolessFloat\")\n  });\n\nCodeMirror.modeInfo.push( {\n            name: \"Fugue SQL\",\n            mime: \"text/x-fsql\",\n            mode: \"sql\"\n          } );\n\nrequire(['notebook/js/codecell'], function(codecell) {\n    codecell.CodeCell.options_default.highlight_modes['magic_text/x-fsql'] = {'reg':[/%%fsql/]} ;\n    Jupyter.notebook.events.on('kernel_ready.Kernel', function(){\n    Jupyter.notebook.get_cells().map(function(cell){\n        if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n    });\n  });\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fugue_notebook import setup\n",
    "\n",
    "setup(is_lab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard SQL Compatible\n",
    "\n",
    "FugueSQL is meant for SQL users to work with Python DataFrame (Pandas, Spark, and Dask). FugueSQL is parsed and then executed on the underlying engine. For example, FugueSQL with Spark is run on top of SparkSQL and PySpark. We'll see non-standard SQL commands later but for now, the important thing to note is that Fugue is compatible with standard SQL. \n",
    "\n",
    "First, we define two Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"col1\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})\n",
    "df2 = pd.DataFrame({\"col1\": [\"A\", \"B\"], \"col3\": [1, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use them in a `%%fsql` cell as seen below. `PRINT` is a FugueSQL keyword to display the first few rows of the DataFrame. Everything else besides `PRINT` is standard SQL. By default, `%%fsql` will run on Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  col2  col3\n",
       "0    A     1     1\n",
       "1    A     2     1\n",
       "2    A     3     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: col1:str,col2:long,col3:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql\n",
    "   SELECT df.col1, df.col2, df2.col3\n",
    "     FROM df\n",
    "LEFT JOIN df2\n",
    "       ON df.col1 = df2.col1\n",
    "    WHERE df.col1 = \"A\"\n",
    "    PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FugueSQL DataFrame in Python\n",
    "\n",
    "FugueSQL can access DataFrames defined in Python. In order to use a DataFrame from a FugueSQL query, we need to use the `YIELD` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"col1\":  [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "SELECT *\n",
    "  FROM df\n",
    " YIELD DATAFRAME AS result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YIELD` will make the variable available in Python. It will be a `FugueDataFrame` where `.native` contains the underlying Pandas, Spark, or Dask DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fugue.dataframe.pandas_dataframe.PandasDataFrame'>\n",
      "  col1  col2\n",
      "0    A     1\n",
      "1    A     2\n",
      "2    A     3\n",
      "3    B     4\n",
      "4    B     5\n"
     ]
    }
   ],
   "source": [
    "print(type(result))\n",
    "print(result.native.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Files\n",
    "\n",
    "In the previous cells, we relied on Python cells to load in the files and then bring them to FugueSQL. We can use FugueSQL to `LOAD` and `SAVE` the files directly. Parquet files are the most preferred method but CSVs and JSON are also supported. They just require some additional arguments. [See this page](https://fugue-tutorials.readthedocs.io/tutorials/fugue_sql/operators.html#load) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Failed to open local file '/tmp/df.parquet'. Detail: [errno 21] Is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb#ch0000062?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mcol1\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mcol2\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m]})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb#ch0000062?line=1'>2</a>\u001b[0m df2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mcol1\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mcol3\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m]})\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb#ch0000062?line=2'>3</a>\u001b[0m df\u001b[39m.\u001b[39;49mto_parquet(\u001b[39m\"\u001b[39;49m\u001b[39m/tmp/df.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb#ch0000062?line=3'>4</a>\u001b[0m df2\u001b[39m.\u001b[39mto_parquet(\u001b[39m\"\u001b[39m\u001b[39m/tmp/df2.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pandas/util/_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pandas/core/frame.py:2835\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2749\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2832\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2833\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2835\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2836\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2837\u001b[0m     path,\n\u001b[1;32m   2838\u001b[0m     engine,\n\u001b[1;32m   2839\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2840\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2841\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2842\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2843\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2844\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pandas/io/parquet.py:420\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    418\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 420\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    421\u001b[0m     df,\n\u001b[1;32m    422\u001b[0m     path_or_buf,\n\u001b[1;32m    423\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    424\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    425\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    426\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    427\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    428\u001b[0m )\n\u001b[1;32m    430\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pandas/io/parquet.py:195\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    187\u001b[0m             table,\n\u001b[1;32m    188\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m         \u001b[39m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mwrite_table(\n\u001b[1;32m    196\u001b[0m             table, path_or_handle, compression\u001b[39m=\u001b[39;49mcompression, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mif\u001b[39;00m handles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:2873\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2871\u001b[0m use_int96 \u001b[39m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[1;32m   2872\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2873\u001b[0m     \u001b[39mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   2874\u001b[0m             where, table\u001b[39m.\u001b[39;49mschema,\n\u001b[1;32m   2875\u001b[0m             filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   2876\u001b[0m             version\u001b[39m=\u001b[39;49mversion,\n\u001b[1;32m   2877\u001b[0m             flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[1;32m   2878\u001b[0m             use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[1;32m   2879\u001b[0m             write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[1;32m   2880\u001b[0m             coerce_timestamps\u001b[39m=\u001b[39;49mcoerce_timestamps,\n\u001b[1;32m   2881\u001b[0m             data_page_size\u001b[39m=\u001b[39;49mdata_page_size,\n\u001b[1;32m   2882\u001b[0m             allow_truncated_timestamps\u001b[39m=\u001b[39;49mallow_truncated_timestamps,\n\u001b[1;32m   2883\u001b[0m             compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2884\u001b[0m             use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_int96,\n\u001b[1;32m   2885\u001b[0m             compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[1;32m   2886\u001b[0m             use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[1;32m   2887\u001b[0m             column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[1;32m   2888\u001b[0m             data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[1;32m   2889\u001b[0m             use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[1;32m   2890\u001b[0m             encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[1;32m   2891\u001b[0m             write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[1;32m   2892\u001b[0m             dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[1;32m   2893\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m writer:\n\u001b[1;32m   2894\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(table, row_group_size\u001b[39m=\u001b[39mrow_group_size)\n\u001b[1;32m   2895\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:912\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, **options)\u001b[0m\n\u001b[1;32m    907\u001b[0m         sink \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_handle \u001b[39m=\u001b[39m filesystem\u001b[39m.\u001b[39mopen(path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    908\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m         \u001b[39m# ARROW-10480: do not auto-detect compression.  While\u001b[39;00m\n\u001b[1;32m    910\u001b[0m         \u001b[39m# a filename like foo.parquet.gz is nonconforming, it\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[39m# shouldn't implicitly apply compression.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m         sink \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_handle \u001b[39m=\u001b[39m filesystem\u001b[39m.\u001b[39;49mopen_output_stream(\n\u001b[1;32m    913\u001b[0m             path, compression\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    914\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     sink \u001b[39m=\u001b[39m where\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/_fs.pyx:681\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.open_output_stream\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/error.pxi:113\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Failed to open local file '/tmp/df.parquet'. Detail: [errno 21] Is a directory"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"col1\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})\n",
    "df2 = pd.DataFrame({\"col1\": [\"A\", \"B\"], \"col3\": [1, 2]})\n",
    "df.to_parquet(\"/tmp/df.parquet\")\n",
    "df2.to_parquet(\"/tmp/df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Failed to open local file '/tmp/df.parquet'. Detail: [errno 21] Is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb#ch0000014?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mcol1\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mcol2\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m]})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb#ch0000014?line=1'>2</a>\u001b[0m df2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mcol1\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mcol3\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m]})\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb#ch0000014?line=2'>3</a>\u001b[0m df\u001b[39m.\u001b[39;49mto_parquet(\u001b[39m\"\u001b[39;49m\u001b[39m/tmp/df.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kevinkho/Work/tutorials/tutorials/quick_look/ten_minutes_sql.ipynb#ch0000014?line=3'>4</a>\u001b[0m df2\u001b[39m.\u001b[39mto_parquet(\u001b[39m\"\u001b[39m\u001b[39m/tmp/df2.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pandas/util/_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pandas/core/frame.py:2835\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2749\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2832\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2833\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2835\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2836\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2837\u001b[0m     path,\n\u001b[1;32m   2838\u001b[0m     engine,\n\u001b[1;32m   2839\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2840\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2841\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2842\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2843\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2844\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pandas/io/parquet.py:420\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    418\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 420\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    421\u001b[0m     df,\n\u001b[1;32m    422\u001b[0m     path_or_buf,\n\u001b[1;32m    423\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    424\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    425\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    426\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    427\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    428\u001b[0m )\n\u001b[1;32m    430\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pandas/io/parquet.py:195\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    187\u001b[0m             table,\n\u001b[1;32m    188\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m         \u001b[39m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mwrite_table(\n\u001b[1;32m    196\u001b[0m             table, path_or_handle, compression\u001b[39m=\u001b[39;49mcompression, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mif\u001b[39;00m handles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:2873\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2871\u001b[0m use_int96 \u001b[39m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[1;32m   2872\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2873\u001b[0m     \u001b[39mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   2874\u001b[0m             where, table\u001b[39m.\u001b[39;49mschema,\n\u001b[1;32m   2875\u001b[0m             filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   2876\u001b[0m             version\u001b[39m=\u001b[39;49mversion,\n\u001b[1;32m   2877\u001b[0m             flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[1;32m   2878\u001b[0m             use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[1;32m   2879\u001b[0m             write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[1;32m   2880\u001b[0m             coerce_timestamps\u001b[39m=\u001b[39;49mcoerce_timestamps,\n\u001b[1;32m   2881\u001b[0m             data_page_size\u001b[39m=\u001b[39;49mdata_page_size,\n\u001b[1;32m   2882\u001b[0m             allow_truncated_timestamps\u001b[39m=\u001b[39;49mallow_truncated_timestamps,\n\u001b[1;32m   2883\u001b[0m             compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2884\u001b[0m             use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_int96,\n\u001b[1;32m   2885\u001b[0m             compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[1;32m   2886\u001b[0m             use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[1;32m   2887\u001b[0m             column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[1;32m   2888\u001b[0m             data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[1;32m   2889\u001b[0m             use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[1;32m   2890\u001b[0m             encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[1;32m   2891\u001b[0m             write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[1;32m   2892\u001b[0m             dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[1;32m   2893\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m writer:\n\u001b[1;32m   2894\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(table, row_group_size\u001b[39m=\u001b[39mrow_group_size)\n\u001b[1;32m   2895\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:912\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, **options)\u001b[0m\n\u001b[1;32m    907\u001b[0m         sink \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_handle \u001b[39m=\u001b[39m filesystem\u001b[39m.\u001b[39mopen(path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    908\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m         \u001b[39m# ARROW-10480: do not auto-detect compression.  While\u001b[39;00m\n\u001b[1;32m    910\u001b[0m         \u001b[39m# a filename like foo.parquet.gz is nonconforming, it\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[39m# shouldn't implicitly apply compression.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m         sink \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_handle \u001b[39m=\u001b[39m filesystem\u001b[39m.\u001b[39;49mopen_output_stream(\n\u001b[1;32m    913\u001b[0m             path, compression\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    914\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     sink \u001b[39m=\u001b[39m where\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/_fs.pyx:681\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.open_output_stream\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fugue/lib/python3.8/site-packages/pyarrow/error.pxi:113\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Failed to open local file '/tmp/df.parquet'. Detail: [errno 21] Is a directory"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"col1\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})\n",
    "df2 = pd.DataFrame({\"col1\": [\"A\", \"B\"], \"col3\": [1, 2]})\n",
    "df.to_parquet(\"/tmp/df.parquet\")\n",
    "df2.to_parquet(\"/tmp/df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "df = LOAD \"/tmp/df.parquet\"\n",
    "df2 = LOAD \"/tmp/df2.parquet\"\n",
    "\n",
    "new =  SELECT df.col1, df.col2, df2.col3\n",
    "         FROM df\n",
    "         LEFT JOIN df2\n",
    "           ON df.col1 = df2.col1 \n",
    "        WHERE df.col1 = \"A\"\n",
    "\n",
    "SAVE OVERWRITE \"/tmp/res.parquet\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Assignment\n",
    "\n",
    "As seen in the previous cell, Fugue simplifies SQL syntax by removing the need for common table expressions (CTEs). They are still supported but FugueSQL users can assign tables to variables with the `=` sign. This reduces a significant amount of boilerplate code SQL practitioners have to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  normalized\n",
       "0    A    1.000000\n",
       "1    B    0.666667\n",
       "2    B    0.833333\n",
       "3    B    1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: col1:str,normalized:double</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql\n",
    "df = LOAD \"/tmp/df.parquet\"\n",
    "\n",
    "max_vals = SELECT col1, MAX(col2) AS max_val\n",
    "             FROM df\n",
    "         GROUP BY col1\n",
    "\n",
    "   SELECT df.col1, \n",
    "          df.col2 / max_vals.max_val AS normalized\n",
    "     FROM df\n",
    "     JOIN max_vals\n",
    "       ON df.col1 = max_vals.col1\n",
    "    PRINT\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymity (Optional)\n",
    "\n",
    "The boilerplate code the SQL introduces can be reduced further by using a FugueSQL featured called anonymity. If no `FROM` clause is used, the last table will be pulled. This way, intermediate steps don't have to be named. The example below has no `FROM` clause. Tables only need to be named if they will be joined downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>max_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  max_val\n",
       "0    A        3\n",
       "1    B        6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: col1:str,max_val:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql\n",
    "LOAD \"/tmp/df.parquet\"\n",
    "\n",
    "SELECT col1, MAX(col2) AS max_val\n",
    " GROUP BY col1\n",
    " PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PRINT` keyword we used earlier actually uses anonymity. The fully written version would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  col2\n",
       "0    A     3\n",
       "1    B     4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: col1:str,col2:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql\n",
    "df = LOAD \"/tmp/df.parquet\"\n",
    "\n",
    "PRINT 2 ROWS FROM df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FugueSQL in Production\n",
    "\n",
    "The `%%fsql` cell magic is meant for iteration inside Jupyter notebooks. To use FugueSQL in scripts, there is a `fsql` class that can be used. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>max_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  max_val\n",
       "0    A        3\n",
       "1    B        6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: col1:str,max_val:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrames()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue_sql import fsql\n",
    "\n",
    "fsql(\"\"\"\n",
    "LOAD \"/tmp/df.parquet\"\n",
    "\n",
    "SELECT col1, MAX(col2) AS max_val\n",
    " GROUP BY col1\n",
    " PRINT\n",
    "\"\"\").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking Python Code\n",
    "\n",
    "In all data computing frameworks, SQL is a second-class citizen often sandwiched between Python code. FugueSQL elevates SQL to be a first class interface that can invoke Python code. We'll show an example below, but for more details about what functions can be used, see the [Fugue in 10 minutes](ten_minutes.ipynb) section. The valid functions for Fugue's `transform()` function will be the same as the ones in FugueSQL.\n",
    "\n",
    "Using Python can often reduce the amount of SQL code that we need to write. For example, let's normalize the column like we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema: *+col2:float\n",
    "def std_dev(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.assign(col2=df['col2']/df['col2'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is defined to handle one group of data at a time. In order to apply it per group, we partition the DataFrame first by group using the `PREPARTITION` and `TRANSFORM` keywords of FugueSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1      col2\n",
       "0    A  1.000000\n",
       "1    B  0.666667\n",
       "2    B  0.833333\n",
       "3    B  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: col1:str,col2:float</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql\n",
    "df = LOAD \"/tmp/df.parquet\"\n",
    "\n",
    "TRANSFORM df PREPARTITION BY col1 USING std_dev\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Engine\n",
    "\n",
    "The strongest feature of FugueSQL is that is get me run on any of the backend engines Fugue supports. Fugue supports Pandas, Spark, Dask, and DuckDB. For operations on a laptop or single machine, DuckDB may give significant improvements over Pandas because it has a query optimizer. \n",
    "\n",
    "For data that is too large to process on a single machine, Spark or Dask can be used. All we need to do is specify the engine in the cell. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1      col2\n",
       "0    A  1.000000\n",
       "1    B  0.666667\n",
       "2    B  0.833333\n",
       "3    B  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: col1:str,col2:float</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql duckdb\n",
    "df = LOAD \"/tmp/df.parquet\"\n",
    "\n",
    "TRANSFORM df PREPARTITION BY col1 USING std_dev\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to run on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1      col2\n",
       "0    A  1.000000\n",
       "1    B  0.666667\n",
       "2    B  0.833333\n",
       "3    B  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: col1:str,col2:float</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql spark\n",
    "df = LOAD \"/tmp/df.parquet\"\n",
    "\n",
    "TRANSFORM df PREPARTITION BY col1 USING std_dev\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For production runs, we can specify the engine in the `.run()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col1  col2\n",
       "0    A   1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: col1:str,col2:float</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrames()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"/tmp/df.parquet\")\n",
    "\n",
    "fsql(\"\"\"\n",
    "SELECT *\n",
    "  FROM df\n",
    " WHERE col1 = 'A' \n",
    "\n",
    "TRANSFORM PREPARTITION BY col1 USING std_dev\n",
    "PRINT\n",
    "\"\"\").run(\"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing Commands (Advanced)\n",
    "\n",
    "One of the weakpoints of SQL is that it doesn't have the grammar to describe distributed computing operations. For example, it's common to `PERSIST` DataFrames in Spark to hold them in memory so that they don't get recomputed.\n",
    "\n",
    "FugueSQL adds keywords such as `PERSIST` and `BROADCAST` to allow users to perform these operations without leaving SQL. In the example below, `df2` will not be recomputed on the Spark and Dask engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cnt\n",
       "0    4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: cnt:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql spark\n",
    "df = LOAD \"/tmp/df.parquet\"\n",
    "\n",
    "df2 = SELECT *\n",
    "        FROM df \n",
    "       WHERE col2 > 2\n",
    "     PERSIST\n",
    "\n",
    "SAVE df2 OVERWRITE \"/tmp/df.parquet\"\n",
    "\n",
    "SELECT COUNT(col2) AS cnt\n",
    "  FROM df2\n",
    " PRINT "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fugue')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
