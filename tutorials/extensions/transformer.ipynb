{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "This is the most used extension in Fugue as it is the extension most tied to application logic. The `Transformer` represents the logic unit executing on partitions of the input dataframe. Because the `Transformer` is concerned with the logic on an individual partition level, it is unaware of the `ExecutionEngine` and is executed on the workers as opposed to the driver.\n",
    "\n",
    "Because the `Transformer` is designed to handle the logic for an individual partition, the partitioning logic is not a concern of `Transformer`. Partitions should be specified in a previous step. Fugue's `partition-transformer` semantic is similar to the `groupby-apply` semantic of Pandas. The main difference is that the partition-transform semantic is scalable to distributed compute.\n",
    "\n",
    "To take full advantage of `Transformers`, it is important to understand [partitioning](../advanced/partition.ipynb) in Fugue.\n",
    "\n",
    "In this tutorial are the methods to define an `Transformer`. There is no preferred method and Fugue makes it flexible for users to choose whatever interface works for them. The four ways are native approach, schema hint, decorator, and the class interface in order of simplicity.\n",
    "\n",
    "## Example Use Cases\n",
    "\n",
    "* **Shift and diff for each group in a timeseries**\n",
    "* **Training seperate ML models for each group**\n",
    "* **Applying different validations for each partition**\n",
    "\n",
    "## Quick Notes on Usage\n",
    "\n",
    "**ExecutionEngine unaware**\n",
    "\n",
    "* `Transformers` are executed on the workers, meaning that they are not unaware of the `ExecutionEngine`.\n",
    "\n",
    "**Acceptable input DataFrame types**\n",
    "\n",
    "* `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "**Acceptable output DataFrame types** \n",
    "\n",
    "* `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "**Further notes**\n",
    "\n",
    "* Notice that `ArrayDataFrame` and other local dataframes can't be used as annotation, you must use `LocalDataFrame`.\n",
    "* `Transformer` requires more explicitness on the output schema compared to `Processor`. This is because schema inference on workers is expensive and unreliable. The schema can be specified through schema hint, decorator, or in the Fugue code.\n",
    "* All valid transformers can be used with Fugue's `transform` in cases where users just want to bring one function to Spark or Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Native Approach\n",
    "\n",
    "The native approach is using a regular function without any edits beyond type annotations for both the input dataframes and output. It is converted to a Fugue extension during runtime. Since schema needs to be explicit, the schema needs to be supplied when the `transformer` is used.\n",
    "\n",
    "The example below also shows how to `partition` a DataFrame before applying a `transformer` on it. This will apply the transformer on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |4    \n",
      "1    |4    \n",
      "Total count: 2\n",
      "\n",
      "PandasDataFrame\n",
      "a:int|b:int\n",
      "-----+-----\n",
      "0    |5    \n",
      "1    |6    \n",
      "Total count: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "\n",
    "def add(df:pd.DataFrame, n=1) -> pd.DataFrame:\n",
    "    df[\"b\"]+=n\n",
    "    return df\n",
    "    \n",
    "def get_top(df:Iterable[Dict[str,Any]]) -> Iterable[Dict[str,Any]]:\n",
    "    yield next(df)\n",
    "    return\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    # with out schema hint you have to specify schema in Fugue code\n",
    "    df = df.transform(add, schema=\"*\").transform(add, schema=\"*\", params=dict(n=2))\n",
    "\n",
    "    # get smallest b of each partition\n",
    "    df.partition(by=[\"a\"], presort=\"b\").transform(get_top, schema=\"*\").show()\n",
    "    # get largest b of each partition\n",
    "    df.partition(by=[\"a\"], presort=\"b DESC\").transform(get_top, schema=\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Schema Hint\n",
    "\n",
    "The schema can also be provided during the function definition through the use of the schema hint comment. Providing it during definition means it does not need to be provided inside the `FugueWorkflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema: *\n",
    "def add(df:pd.DataFrame, n=1) -> pd.DataFrame:\n",
    "    df[\"b\"]+=n\n",
    "    return df\n",
    "    \n",
    "# schema: *\n",
    "def get_top(df:Iterable[Dict[str,Any]]) -> Iterable[Dict[str,Any]]:\n",
    "    yield next(df)\n",
    "    return\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df = df.transform(add).transform(add, params=dict(n=2)) # see how parameters are set\n",
    "    df.partition(by=[\"a\"], presort=\"b\").transform(get_top).show()\n",
    "    df.partition(by=[\"a\"], presort=\"b DESC\").transform(get_top).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Hint Syntax\n",
    "\n",
    "There is a special syntax for schema only available to `Transformers` Please read [this](https://triad.readthedocs.io/en/latest/api/triad.collections.html#triad.collections.schema.Schema.transform) for detailed syntax, here we only show some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "\n",
    "# schema: *,c:int\n",
    "def with_c(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"c\"]=1\n",
    "    return df\n",
    "\n",
    "# schema: *-b\n",
    "def drop_b(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.drop(\"b\", axis=1)\n",
    "\n",
    "# schema: *~b,c\n",
    "def drop_b_c_if_exists(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.drop([\"b\",\"c\"], axis=1, errors='ignore')\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df = df.transform(with_c)\n",
    "    df.show()\n",
    "    df = df.transform(drop_b)\n",
    "    df.show()\n",
    "    df = df.transform(drop_b_c_if_exists)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator Approach\n",
    "\n",
    "The decorator approach also has the special schema syntax, but it can also take a function that generates the schema. This can be used to create new column names or types based on transformer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "a:int|b:int|c:int\n",
      "-----+-----+-----\n",
      "0    |1    |1    \n",
      "0    |2    |1    \n",
      "1    |3    |1    \n",
      "1    |1    |1    \n",
      "Total count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import transformer\n",
    "\n",
    "# df is the zipped DataFrames, **kwargs is the parameters passed in from Fugue\n",
    "# the syntax below is equivalent to @transformer(\"*,c:int\") \n",
    "@transformer(lambda df, **kwargs: df.schema+\"c:int\") \n",
    "def with_c(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"c\"]=1\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df = df.transform(with_c)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Approach\n",
    "\n",
    "All the previous methods are just wrappers of the interface approach. They cover most of the use cases and simplify the usage. But for certain cases, you should implement interface, for example:\n",
    "\n",
    "* Your output schema needs partition information, such as partition keys, schema, and current values of the keys\n",
    "* You have an expensive but common initialization step for processing each logical partition, this should happen when initializaing physical partition\n",
    "\n",
    "The biggest advantage of interface approach is that you can customize pyhisical partition level initialization, and you have all the up-to-date context variables to use.\n",
    "\n",
    "In the interface approach, type annotations are not necessary, but again, it's good practice to have them.\n",
    "\n",
    "The following case focuses on performance comparison, to see how to use context variables, read [CoTransfromer example](./cotransformer.ipynb#Interface-Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import Transformer, FugueWorkflow, PandasDataFrame, DataFrame, LocalDataFrame\n",
    "from triad.collections import Schema\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def expensive_init(sec=5):\n",
    "    sleep(sec)\n",
    "\n",
    "def helper(ct=20) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 3)), columns=list('abc'))\n",
    "\n",
    "class Median(Transformer):\n",
    "    # this is invoked on driver side\n",
    "    def get_output_schema(self, df):\n",
    "        return df.schema + (self.params.get_or_throw(\"col\", str),float)\n",
    "    \n",
    "    # on initialization of the physical partition\n",
    "    def on_init(self, df: DataFrame) -> None:\n",
    "        self.col = self.params.get_or_throw(\"col\", str)\n",
    "        expensive_init(self.params.get(\"sec\",0))\n",
    "        \n",
    "    def transform(self, df):\n",
    "        pdf = df.as_pandas()\n",
    "        pdf[self.col]=pdf[\"b\"].median()\n",
    "        return PandasDataFrame(pdf)\n",
    "        \n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.create(helper).partition(by=[\"a\"]).transform(Median, params={\"col\":\"m\", \"sec\": 1}).show(rows=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we set `self.col` in `on_init`, it's better to set it in `on_init` or `transform`. It's better not to set it in `get_output_schema` because that will need to be serialized and send to each workers if using a distributed engine, serialization can fail for some value types.\n",
    "\n",
    "In order to show the benefit of `on_init` we also create an interfaceless version (which is a lot simpler), but you have to call `expensive_init` in that function for each logical partition. Also, in the run function, we set `num=2` to show the effect. So for `Median` transformer, the `expensive_init` will be called at most twice, but for `median` it will be called for more times.\n",
    "\n",
    "Notice, the numbers may be off if you run this on binder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_spark import SparkExecutionEngine\n",
    "from timeit import timeit\n",
    "\n",
    "# schema: *, m:double\n",
    "def median(df:pd.DataFrame, sec=0) -> pd.DataFrame:\n",
    "    expensive_init(sec)\n",
    "    df[\"m\"]=df[\"b\"].median()\n",
    "    return df\n",
    "\n",
    "def run(engine, interfaceless, sec):\n",
    "    with FugueWorkflow(engine) as dag:\n",
    "        df = dag.create(helper)\n",
    "        if interfaceless:\n",
    "            df.partition(by=[\"a\"], num=2).transform(median, params={\"sec\": sec}).show(rows=100)\n",
    "        else:\n",
    "            df.partition(by=[\"a\"], num=2).transform(Median, params={\"col\":\"m\", \"sec\": sec}).show(rows=100)\n",
    "    \n",
    "engine = SparkExecutionEngine()\n",
    "print(timeit(lambda: run(engine, True, 1), number=1))\n",
    "print(timeit(lambda: run(engine, False, 1), number=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}