{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processor\n",
    "\n",
    "`Processor` represents the logic unit executing on driver on the **entire** input dataframes.\n",
    "\n",
    "**Input can be a single** [DataFrames](x-like.ipynb#DataFrames)\n",
    "\n",
    "**Alternatively, acceptable input DataFrame types are**: `DataFrame`, `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "**Acceptable output DataFrame types are**: `DataFrame`, `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "**Before input DataFrames** you can have a parameter with `ExecutionEngine` annotation so Fugue will pass the current `ExecutionEngine` to you\n",
    "\n",
    "Notice\n",
    "\n",
    "* `ArrayDataFrame` and other local dataframes can't be used as annotation, you must use `LocalDataFrame` or `DataFrme`\n",
    "* If output type is NOT one of `DataFrame`, `LocalDataFrame` or `pd.DataFrame`, the output schema is unknown, so you must specify that.\n",
    "* `DataFrame` or `DataFrames` are the recommended input/output types, all other acceptable types are variations of `LocalDataFrame` that means the dataset will be materialized and brought to driver to process.\n",
    "* `Iterable` like input may have different execution plans to bring data to driver, in some cases it can be less optimal, you must be careful.\n",
    "\n",
    "\n",
    "## Native Approach\n",
    "\n",
    "The simplest way, with no dependency on Fugue. You just need to have acceptable annotations on input dataframes and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "\n",
    "# the output is pd.DataFrame, fugue can get schema from it\n",
    "def add1(df:pd.DataFrame, n=1) -> pd.DataFrame:\n",
    "    df[\"b\"]+=n\n",
    "    return df\n",
    "\n",
    "# the output has no schema info, so you must specify schema in fugue code\n",
    "# in practice, it's rare to use such output type for a processor\n",
    "def add2(df:List[Dict[str,Any]], n=1) -> Iterable[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        row[\"b\"]+=n\n",
    "        yield row\n",
    "\n",
    "def concat(df1:pd.DataFrame, df2:pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.concat([df1,df2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df.process(add1, params={\"n\":2}).show()\n",
    "    dag.process(df,using=add1,params={\"n\":2}).show() # == above\n",
    "    df.process(add2, schema=\"a:int,b:int\", params={\"n\":2}).show()\n",
    "    dag.process(df,df, using=concat).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important to know another use case: with `ExecutionEngine`. **This is how you write native Spark code inside Fugue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import ExecutionEngine, DataFrame\n",
    "from fugue_spark import SparkExecutionEngine, SparkDataFrame\n",
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "\n",
    "# pay attention to the input and output annotations, they are both general DataFrame\n",
    "def add(e:ExecutionEngine, df:DataFrame, temp_name=\"x\") -> DataFrame:\n",
    "    assert isinstance(e,SparkExecutionEngine) # this extension only works with SparkExecutionEngine\n",
    "    df = e.to_df(df) # to make sure df is SparkDataFrame, or conversion is done here\n",
    "    df.native.createOrReplaceTempView(temp_name)  # df.native is spark dataframe\n",
    "    sdf = e.spark_session.sql(\"select a,b+1 as b from \"+temp_name)  # this is how you get spark session\n",
    "    return SparkDataFrame(sdf) # you must wrap as Fugue SparkDataFrame to return\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df.process(add, params={\"temp_name\":\"y\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also important to know how to use `DataFrames` as input annotation. Because this is the only way to be **dynamic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "from fugue import DataFrames, DataFrame\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "def concat(dfs:DataFrames) -> pd.DataFrame:\n",
    "    pdfs = [df.as_pandas() for df in dfs.values()]\n",
    "    return pd.concat(pdfs).reset_index(drop=True) # Fugue can't take pandas dataframe with special index\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1]],\"a:int,b:int\")\n",
    "    df2 = dag.df([[0,2],[1,3]],\"a:int,b:int\")\n",
    "    df3 = dag.df([[1,1]],\"a:int,b:int\")\n",
    "    dag.process(df1,using=concat).show()\n",
    "    dag.process(df1,df2,using=concat).show()\n",
    "    dag.process(df1,df2,df3,using=concat).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Schema Hint\n",
    "\n",
    "Notice if you are using `DataFrame`, `LocalDataFrame` or `pd.DataFrame` as the output type, you must not have type hint. And the best practice is to use `DataFrame` as the output type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "\n",
    "# schema: a:int, b:int\n",
    "def add(df:List[Dict[str,Any]], n=1) -> Iterable[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        row[\"b\"]+=n\n",
    "        yield row\n",
    "\n",
    "\n",
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.df([[0,1]],\"a:int,b:int\").process(add).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator Approach\n",
    "\n",
    "There is no obvious advantage to use decorator for `Processor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import processor, FugueWorkflow\n",
    "import pandas as pd\n",
    "\n",
    "@processor(\"a:int, b:int\")\n",
    "def add(df:List[Dict[str,Any]], n=1) -> Iterable[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        row[\"b\"]+=n\n",
    "        yield row\n",
    "\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.df([[0,1]],\"a:int,b:int\").process(add).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Approach\n",
    "\n",
    "All the previous methods are just wrappers of the interface approach. They cover most of the use cases and simplify the usage. But if you want to get all execution context such as partition information, use interface.\n",
    "\n",
    "In the interface approach, type annotations are not necessary, but again, it's good practice to have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow, Processor, DataFrames, DataFrame\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Partitioner(Processor):\n",
    "    def process(self, dfs:DataFrames) -> DataFrame:\n",
    "        assert len(dfs)==1\n",
    "        engine = self.execution_engine\n",
    "        partion = self.partition_spec\n",
    "        return engine.repartition(dfs[0], partition_spec = partion)\n",
    "\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df([[0,1],[0,3],[1,2],[1,1]],\"a:int,b:int\")\n",
    "    # see the output is sorted by b, partition is passed into Partitioner as partition_spec\n",
    "    df.partition(num=1, presort=\"b\").process(Partitioner).show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
