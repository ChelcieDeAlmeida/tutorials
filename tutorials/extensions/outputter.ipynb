{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputter\n",
    "\n",
    "`Outputter` represents the logic unit executing on driver on the **entire** input dataframes WITHOUT output. It's called Outputter because normally this step is to output the data to certain location or print on console.\n",
    "\n",
    "**Input can be a single** [DataFrames](x-like.ipynb#DataFrames)\n",
    "\n",
    "**Alternatively, acceptable input DataFrame types are**: `DataFrame`, `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "**Output annotation must be None** \n",
    "\n",
    "**Before input DataFrames** you can have a parameter with `ExecutionEngine` annotation so Fugue will pass the current `ExecutionEngine` to you\n",
    "\n",
    "Notice\n",
    "\n",
    "* `ArrayDataFrame` and other local dataframes can't be used as annotation, you must use `LocalDataFrame` or `DataFrme`\n",
    "* Variations of `LocalDataFrame` will bring the entire dataset onto driver, for an Outputter this might be an expected operation, but you need to be careful.\n",
    "* `Iterable` like input may have different exeuction plans to bring data to driver, in some cases it can be less optimial (slower), you need to be careful.\n",
    "\n",
    "\n",
    "## Native Approach\n",
    "\n",
    "The simplest way, with no dependency on Fugue. You just need to have acceptable annotations on the input dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "\n",
    "def out(df:List[List[Any]], n=1) -> None:\n",
    "    for i in range(n):\n",
    "        print(df)\n",
    "\n",
    "def out2(df1:pd.DataFrame, df2:List[List[Any]]) -> None:\n",
    "    print(df1)\n",
    "    print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df.output(out, params={\"n\":2})\n",
    "    dag.output(df,using=out,params={\"n\":2}) # == above\n",
    "    \n",
    "    dag.output(df,df,using=out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important to know another use case: with `ExecutionEngine`. **This is how you write native Spark code inside Fugue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import ExecutionEngine, DataFrame\n",
    "from fugue_spark import SparkExecutionEngine, SparkDataFrame\n",
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "\n",
    "# pay attention to the input annotations\n",
    "def out(e:ExecutionEngine, df:DataFrame) -> None:\n",
    "    assert isinstance(e,SparkExecutionEngine) # this extension only works with SparkExecutionEngine\n",
    "    df = e.to_df(df) # to make sure df is SparkDataFrame, or conversion is done here\n",
    "    df.native.show()\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df([[0,1],[0,2],[1,3],[1,1]],\"a:int,b:int\")\n",
    "    df.output(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also important to know how to use `DataFrames` as input annotation. Because this is the way to be **dynamic** on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "from fugue import DataFrames, DataFrame\n",
    "\n",
    "def out(dfs:DataFrames) -> None:\n",
    "    for k, v in dfs.items():\n",
    "        v.show(title=k)\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1]],\"a:int,b:int\")\n",
    "    df2 = dag.df([[0,2],[1,3]],\"a:int,b:int\")\n",
    "    df3 = dag.df([[1,1]],\"a:int,b:int\")\n",
    "    dag.output(df1,df2,df3,using=out)\n",
    "    dag.output(dict(x=df1,y=df2,z=df3),using=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator Approach\n",
    "\n",
    "There is no obvious advantage to use decorator for `Outputter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import outputter, FugueWorkflow\n",
    "import pandas as pd\n",
    "\n",
    "@outputter()\n",
    "def out(df:List[List[Any]], n=1) -> None:\n",
    "    for i in range(n):\n",
    "        print(df)\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.df([[0,1]],\"a:int,b:int\").output(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Approach\n",
    "\n",
    "All the previous methods are just wrappers of the interface approach. They cover most of the use cases and simplify the usage. But if you want to get all execution context such as partition information, use interface.\n",
    "\n",
    "In the interface approach, type annotations are not necessary, but again, it's good practice to have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow, Outputter, DataFrames\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Save(Outputter):\n",
    "    def process(self, dfs:DataFrames) -> None:\n",
    "        assert len(dfs)==1\n",
    "        assert isinstance(self.execution_engine, SparkExecutionEngine)\n",
    "        session = self.execution_engine.spark_session\n",
    "        # we get the partition information from Outputter\n",
    "        by = self.partition_spec.partition_by\n",
    "        df = self.execution_engine.to_df(dfs[0])\n",
    "        path = self.params.get_or_throw(\"path\",str)\n",
    "        df.native.write.partitionBy(*by).format(\"parquet\").mode(\"overwrite\").save(path)\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df([[0,1],[0,3],[1,2],[1,1]],\"a:int,b:int\")\n",
    "    df.partition(by=[\"a\"]).output(Save, params=dict(path=\"/tmp/x.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Real Cases to Consider\n",
    "\n",
    "In the following cases, Fugue does not have built in extensions, but it will be very easy to write them by yourselves\n",
    "\n",
    "* **Jupyter notebook pretty printer**, jupyter has it's own way to pretty print tables. You can look at how `DataFrame.show()` is implemented and create your own modified version to pretty print the head n rows using jupyter API. Look at [this example](example_covid19.ipynb#First-of-all,-I-want-to-make-the-experiment-environment-more-friendly)\n",
    "* **Spark save table**, this is as simple as a few lines of code, but for some users, this can be extremely useful.\n",
    "* **Unit test assertion**, you can take in dataframes and make assertion using your own logic. In this way, it's much easier to unit test your Fugue workflow because everything can be in one dag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
