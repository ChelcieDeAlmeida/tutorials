{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creator\n",
    "\n",
    "`Creator` represents the logic unit to generate a DataFrame.\n",
    "\n",
    "## Quick Notes\n",
    "\n",
    "**ExecutionEngine aware**\n",
    "\n",
    "* Creators run on the driver so they are aware of the ExecutionEngine being used. Passing a parameter with the `ExecutionEngine` annotation will pass in the current `ExecutionEngine`. There is an example of this later.\n",
    "\n",
    "**Does not take in DataFrames as arguments**\n",
    "\n",
    "* Other parameters are fine to add.\n",
    "\n",
    "**Acceptable output DataFrame types**\n",
    "\n",
    "* `DataFrame`, `LocalDataFrame`, `pd.DataFrame`, `List[List[Any]]`, `Iterable[List[Any]]`, `EmptyAwareIterable[List[Any]]`, `List[Dict[str, Any]]`, `Iterable[Dict[str, Any]]`, `EmptyAwareIterable[Dict[str, Any]]`\n",
    "\n",
    "**Schema must be known**\n",
    "\n",
    "* If output type is NOT one of `DataFrame`, `LocalDataFrame` or `pd.DataFrame`, the output schema must be specified because it is not known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaceless\n",
    "\n",
    "The native approach is using a regular function without any edits beyond type annotations. It is converted to a Fugue extention during runtime. In the example below we have two create functions. The first one has an output type of `pd.DataFrame`, which means that the schema is already known. The second one has an output type of `List[List[Any]]`, which does not know schema and thus has to be provided during the `create` call inside `FugueWorkflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "a:long\n",
      "------\n",
      "2     \n",
      "Total count: 1\n",
      "\n",
      "ArrayDataFrame\n",
      "a:int\n",
      "-----\n",
      "2    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "\n",
    "def create1(n=1) -> pd.DataFrame:\n",
    "    return pd.DataFrame([[n]],columns=[\"a\"])\n",
    "\n",
    "# schema is not known so it has to be provided later\n",
    "def create2(n=1) -> List[List[Any]]:\n",
    "    return [[n]]\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.create(create1, params={\"n\":2}).show()\n",
    "    dag.create(create2, schema=\"a:int\", params={\"n\":2}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema can also be provided during the function definition through the use of the schema hint comment. Providing it during definition means it does not need to be provided inside the `FugueWorkflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayDataFrame\n",
      "a:int\n",
      "-----\n",
      "1    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema: a:int\n",
    "def create2(n=1) -> List[List[Any]]:\n",
    "    return [[n]]\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.create(create2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator Approach\n",
    "\n",
    "There is no obvious advantage to use the decorator approach for defining a `Creator`. In general, the decorator is good is the schema is too long to type out as a comment in one line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayDataFrame\n",
      "a:int\n",
      "-----\n",
      "1    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import creator\n",
    "\n",
    "@creator(\"a:int\")\n",
    "def create(n=1) -> List[List[Any]]:\n",
    "    return [[n]]\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.create(create).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Approach (Advanced)\n",
    "\n",
    "All the previous methods are just wrappers of the interface approach. They cover most of the use cases and simplify the usage. But if you want to get all execution context such as partition information, use interface approach.\n",
    "\n",
    "In the interface approach, type annotations are not necessary, but again, it's good practice to have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayDataFrame\n",
      "a:int\n",
      "-----\n",
      "1    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import Creator, DataFrame\n",
    "\n",
    "class Array(Creator):\n",
    "    def create(self) -> DataFrame:\n",
    "        engine = self.execution_engine\n",
    "        n = self.params.get_or_throw(\"n\",int)\n",
    "        return engine.to_df([[n]],\"a:int\")\n",
    "\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.create(Array, params=dict(n=1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ExecutionEngine\n",
    "\n",
    "In some cases, the `Creator` has to be aware of the `ExecutionEngine`. **This is an example of how to write native Spark code inside Fugue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "a:int\n",
      "-----\n",
      "2    \n",
      "Total count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import ExecutionEngine\n",
    "from fugue_spark import SparkExecutionEngine, SparkDataFrame\n",
    "\n",
    "# pay attention to the input and output annotations, they are both general DataFrame\n",
    "def create(e:ExecutionEngine, n=1) -> DataFrame:\n",
    "    assert isinstance(e,SparkExecutionEngine) # this extension only works with SparkExecutionEngine\n",
    "    sdf= e.spark_session.createDataFrame([[n]], schema=\"a:int\")  # this is how you get spark session\n",
    "    return SparkDataFrame(sdf) # you must wrap as Fugue SparkDataFrame to return\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    dag.create(create, params={\"n\":2}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "These are common cases to use a `Creator`.\n",
    "\n",
    "* **Reading special data sources** like constructing a DataFrame using an API.\n",
    "* **Querying a database** using `pyodbc` and returning a DataFrame\n",
    "* **Create mock data for unit tests**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}