{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9777b881",
   "metadata": {},
   "source": [
    "To use the transformer, you will need to directly inherit the base class because it needs a lot of customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da001f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Any\n",
    "\n",
    "from fugue import DataFrame, LocalDataFrame, Transformer, Schema, ArrayDataFrame\n",
    "\n",
    "class Pivot(Transformer):\n",
    "    def get_output_schema(self, df:DataFrame):\n",
    "        self.index_cols = self.partition_spec.partition_by\n",
    "        self.key_col = self.params.get_or_throw(\"key_col\", str)\n",
    "        self.value_col = self.params.get_or_throw(\"value_col\", str)\n",
    "        self.values = self.params.get_or_throw(\"values\", list)\n",
    "        self.value_type = df.schema[self.value_col].type\n",
    "        \n",
    "        sub_schema = Schema([(v, self.value_type) for v in self.values])\n",
    "        return df.schema.extract(self.index_cols) + sub_schema\n",
    "    \n",
    "    def validate_on_compile(self) -> None:\n",
    "        assert len(self.partition_spec.partition_by) > 0\n",
    "        \n",
    "    def transform(self, df:LocalDataFrame):\n",
    "        pdf = df[[self.key_col, self.value_col]].as_pandas()\n",
    "        res = self.cursor.key_value_array\n",
    "        kv = dict(zip(pdf[self.key_col], pdf[self.value_col]))\n",
    "        values = [kv.get(k, None) for k in self.values]\n",
    "        return ArrayDataFrame([res+values], self.output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f343280",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(\n",
    "    id = [1,1,2,2,2],\n",
    "    id2 = [10,10,20,20,20],\n",
    "    key = [\"a\",\"b\",\"a\",\"b\",\"c\"],\n",
    "    v1 = [3,1,2,3,4],\n",
    "    v2 = [30,10,20,30,40],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2b9545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  a  b    c\n",
       "0   1  3  1  NaN\n",
       "1   2  2  3  4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue import transform\n",
    "\n",
    "transform(\n",
    "    df, Pivot,\n",
    "    params=dict(key_col=\"key\", value_col=\"v1\", values=[\"a\",\"b\",\"c\"]),\n",
    "    partition={\"by\":\"id\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fbf571",
   "metadata": {},
   "source": [
    "Feel free to add engine parameter to `transform` to make it run distributedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4989f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow, WorkflowDataFrame, module\n",
    "\n",
    "@module(as_method=True)\n",
    "def pivot(df:WorkflowDataFrame, index, key_col, value_col, values) -> WorkflowDataFrame:\n",
    "    return df.partition(by=index).transform(Pivot, params=dict(key_col=key_col, value_col=value_col, values=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f29bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "id:long|a:long|b:long|c:long\n",
      "-------+------+------+------\n",
      "1      |3     |1     |NULL  \n",
      "2      |2     |3     |4     \n",
      "Total count: 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrames()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag = FugueWorkflow()\n",
    "dag.df(df).pivot(\"id\", \"key\", \"v1\", [\"a\",\"b\",\"c\"]).show()\n",
    "\n",
    "dag.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0599c266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require([\"codemirror/lib/codemirror\"]);\n",
       "function set(str) {\n",
       "    var obj = {}, words = str.split(\" \");\n",
       "    for (var i = 0; i < words.length; ++i) obj[words[i]] = true;\n",
       "    return obj;\n",
       "  }\n",
       "var fugue_keywords = \"fill hash rand even presort persist broadcast params process output outtransform rowcount concurrency prepartition zip print title save append parquet csv json single checkpoint weak strong deterministic yield connect sample seed take sub callback dataframe file\";\n",
       "CodeMirror.defineMIME(\"text/x-fsql\", {\n",
       "    name: \"sql\",\n",
       "    keywords: set(fugue_keywords + \" add after all alter analyze and anti archive array as asc at between bucket buckets by cache cascade case cast change clear cluster clustered codegen collection column columns comment commit compact compactions compute concatenate cost create cross cube current current_date current_timestamp database databases data dbproperties defined delete delimited deny desc describe dfs directories distinct distribute drop else end escaped except exchange exists explain export extended external false fields fileformat first following for format formatted from full function functions global grant group grouping having if ignore import in index indexes inner inpath inputformat insert intersect interval into is items join keys last lateral lazy left like limit lines list load local location lock locks logical macro map minus msck natural no not null nulls of on optimize option options or order out outer outputformat over overwrite partition partitioned partitions percent preceding principals purge range recordreader recordwriter recover reduce refresh regexp rename repair replace reset restrict revoke right rlike role roles rollback rollup row rows schema schemas select semi separated serde serdeproperties set sets show skewed sort sorted start statistics stored stratify struct table tables tablesample tblproperties temp temporary terminated then to touch transaction transactions transform true truncate unarchive unbounded uncache union unlock unset use using values view when where window with\"),\n",
       "    builtin: set(\"date datetime tinyint smallint int bigint boolean float double string binary timestamp decimal array map struct uniontype delimited serde sequencefile textfile rcfile inputformat outputformat\"),\n",
       "    atoms: set(\"false true null\"),\n",
       "    operatorChars: /^[*\\/+\\-%<>!=~&|^]/,\n",
       "    dateSQL: set(\"time\"),\n",
       "    support: set(\"ODBCdotTable doubleQuote zerolessFloat\")\n",
       "  });\n",
       "\n",
       "CodeMirror.modeInfo.push( {\n",
       "            name: \"Fugue SQL\",\n",
       "            mime: \"text/x-fsql\",\n",
       "            mode: \"sql\"\n",
       "          } );\n",
       "\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "    codecell.CodeCell.options_default.highlight_modes['magic_text/x-fsql'] = {'reg':[/%%fsql/]} ;\n",
       "    Jupyter.notebook.events.on('kernel_ready.Kernel', function(){\n",
       "    Jupyter.notebook.get_cells().map(function(cell){\n",
       "        if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "    });\n",
       "  });\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fugue_notebook import setup\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c83f889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  a  b    c\n",
       "0   1  3  1  NaN\n",
       "1   2  2  3  4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>schema: id:long,a:long,b:long,c:long</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%fsql\n",
    "SUB df USING pivot(index='id', key_col='key', value_col='v1', values=['a','b','c'])\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd6100",
   "metadata": {},
   "source": [
    "This is also related with a concept chain that is difficult to understand (we don’t most average users to understand) that is transformer (worker side extension) -> processor (driver side extension) -> module (workflow level extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d94f1",
   "metadata": {},
   "source": [
    "We expect most users will just use transformers to solve problems and not to implement any Fugue interface. But we do have a sophisticated system behind the simple scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb06a2e7",
   "metadata": {},
   "source": [
    "Also, if you want to use Spark as the backend, you should let Fugue use pandas udf to accelerate, that can be much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6850d808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/16 11:11:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e81b2707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, a: bigint, b: bigint, c: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(\n",
    "    df, Pivot,\n",
    "    params=dict(key_col=\"key\", value_col=\"v1\", values=[\"a\",\"b\",\"c\"]),\n",
    "    partition={\"by\":\"id\"},\n",
    "    engine=spark, engine_conf={\"fugue.spark.use_pandas_udf\":True}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
