{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation for Fugue\n",
    "\n",
    "Before we talk about Fugue, we need to first go through the scenarios that make data practitioners use distributed computed frameworks such as Spark and Dask. This will help us understand the \"why\" behind Fugue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data is small enough to fit on a laptop, it's easy for data practitioners to iterate on data projects. Most commonly, data practitioners use pandas and numpy for their data analysis and feature engineering needs. These tools go well with scikit-learn to provide a stack capable of handling the end-to-end machine learning pipeline. This works great, until data becomes too big to fit on a single machine. At this point, there are a couple of options to scale pandas-based code.\n",
    "\n",
    "1. Sampling\n",
    "2. Increasing Hardware Vertically\n",
    "3. Utilizing Distributed Compute Frameworks (Spark and Dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and Increasing Hardware\n",
    "\n",
    "Sampling allows users to use the exact same codebase without modifications, but this method stops working when the cide has to be applied to a bigger dataset. For example, we can train a machine learning model on a subset of data, but we may still need to generate predictions for all the data coming in, and that dataset may be too big. Running the predictions by batch sequentially does not scale well.\n",
    "\n",
    "We can scale hardware vertically, meaning adding more resources for Pandas, but the real issue is that Pandas does not scale well. The primary reason is that Pandas is single core, and does not take advantage of the available compute resources all of the time. A lot of operations also generate [intermediate copies](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#scaling-to-large-datasets) of data, causing more memory than expected. \n",
    "\n",
    "Vertical scaling also involves having a powerful machine consistently on, leading to underutilization during the less intensive operations. Horizontal scaling, on the other hand, involves spinning up multiple machines and distributing the compute job across them. This is what is called distributed computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Distributed Compute\n",
    "\n",
    "This leads us to frameworks such as Spark and Dask. These frameworks allow us to split compute jobs across multiple machines. But of course, using them would require us to alter the code base. Dask is friendlier than Spark in this sense because the Dask DataFrame is comprised of multiple Pandas DataFrames, while the Spark DataFrame is an entirely different class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fugue Transform\n",
    "\n",
    "The question then becomes what is the easiest way to take advantage of Spark and Dask with minimal changes to the pandas-based code. Fugue is an abstraction layer designed to provide a seamless transition between local compute to distributed compute. Fugue allows users to take advantage of the Spark and Dask computation engines, while writing Python, Pandas, and SQL code. This allows users to focus on the problems they are trying to solve, rather than learning a new framework for the job.\n",
    "\n",
    "The most basic way Fugue can be used to scale Pandas based code to Spark is the `transform` function. We'll copy the code from the [Sklearn Linear Regression example](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = pd.DataFrame({\"x_1\": [1, 1, 2, 2], \"x_2\":[1, 2, 2, 3]})\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we then wrap it in a function to be used for production. This function is still written in Pandas. We can easily test it on the `input_df` that we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x_1</th>\n      <th>x_2</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>3</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>3</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>6</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>6</td>\n      <td>21.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   x_1  x_2  predicted\n0    3    3       12.0\n1    4    3       13.0\n2    6    6       21.0\n3    6    6       21.0"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(df: pd.DataFrame, model: LinearRegression) -> pd.DataFrame:\n",
    "    return df.assign(predicted=model.predict(df))\n",
    "\n",
    "input_df = pd.DataFrame({\"x_1\": [3, 4, 6, 6], \"x_2\":[3, 3, 6, 6]})\n",
    "\n",
    "# test the function\n",
    "predict(input_df.copy(), reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we bring it to Spark using Fugue. Fugue has a function called `transform` that takes in a DataFrame and applies a function to it. We'll explain the inputs that go into this function in a bit (though you probably already have a good clue). The important thing to notice is that we did not make modifications to the Pandas based function in order to use it Spark. This function can now scale to big datasets through the Spark engine.\n",
    "\n",
    "Even if there is no cluster available, the SparkExecutionEngine will start a local Spark instance and parallelize the jobs with all cores of the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/31 23:13:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# create Spark session for next cells\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "|x_1|x_2|predicted|\n",
      "+---+---+---------+\n",
      "|  3|  3|     12.0|\n",
      "|  4|  3|     13.0|\n",
      "|  6|  6|     21.0|\n",
      "|  6|  6|     21.0|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import transform\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "result = transform(\n",
    "    input_df,\n",
    "    predict,\n",
    "    schema=\"*,predicted:double\",\n",
    "    params=dict(model=reg),\n",
    "    engine=SparkExecutionEngine()\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two arguments of the function are the DataFrame to operate on and the function to use. The `input_df` can either be a Pandas DataFrame or a Spark DataFrame. The engine then dictates what engine to use for the computation. Because we supplied a Pandas DataFrame with the SparkExecutionEngine, that DataFrame was converted to be used in Spark. The output of this function is a Spark DataFrame because the `engine` used was the `SparkExecutionEngine`. Supplying no engine uses the pandas-based `NativeExecutionEngine`. Fugue also has a `DaskExecutionEngine` available. \n",
    "\n",
    "The other two arguments are the schema and parameter. The schema is a hard requirement in distributed computing frameworks, so we need to supply the output schema of the operation. `params` is a dictionary that contains other inputs into the function. In this case, we passed in the regression model to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With that, we have shown the use-case of Fugue in scaling Python and Pandas-written code to Spark. It can be done in very few lines of code, without comprimising the existing code base. In the next sections, we'll see other features Fugue has to offer, and the other ways it simplifies using distributed compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Equivalent of Transform\n",
    "\n",
    "If you are wondering how `transform` compares to implementing the same logic in Spark, below is an example of how the Pandas function would be implemented in Spark if you did it yourself. This implementation uses the Spark's `mapInPandas` method. Note how the schema has to be handled inside `run_predict`. This is the schema requirement we mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "|x_1|x_2|predicted|\n",
      "+---+---+---------+\n",
      "|  3|  3|     12.0|\n",
      "|  4|  3|     13.0|\n",
      "|  6|  6|     21.0|\n",
      "|  6|  6|     21.0|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, Any, Union\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "def predict_wrapper(dfs: Iterator[pd.DataFrame], model):\n",
    "    for df in dfs:\n",
    "        yield predict(df, model)\n",
    "\n",
    "def run_predict(sdf, model):\n",
    "    schema = StructType(list(sdf.schema.fields))\n",
    "    schema.add(StructField(\"predicted\", DoubleType()))\n",
    "    return sdf.mapInPandas(lambda dfs: predict_wrapper(dfs, model), \n",
    "                           schema=schema)\n",
    "\n",
    "# conversion\n",
    "if isinstance(input_df, pd.DataFrame):\n",
    "    sdf = spark_session.createDataFrame(input_df.copy())\n",
    "else:\n",
    "    sdf = input_df.copy()\n",
    "\n",
    "result = run_predict(sdf, reg)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very easy to see why it becomes very difficult to bring a Pandas codebase to Spark with this approach. We had to define two additional functions in the `predict_wrapper` and the `run_predict` to bring it to Spark. If this had to be done for tens of functions, it can easily fill the codebase with boilerplate code, rather than the logic."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('fugue-tutorials': conda)",
   "metadata": {
    "interpreter": {
     "hash": "131b24c7e1bb8763ab2b04d5b6d98a68c7b3a823a2a57c5722935f7690890f70"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "f7f9294720e464cd08733c6cd5cfe1a4599977fa03668bc63f2dfd97f1a61807"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}