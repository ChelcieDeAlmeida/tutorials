{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoupling Logic and Execution\n",
    "\n",
    "So far we've used Fugue's `transform()` function to port Pandas code to Spark without any rewrites. The `transform()` function is very flexible so it can handle functions with varying input and output types.\n",
    "\n",
    "Decoupling logic and execution is one of the primary motivations of Fugue. This is meant to solve the following problems:\n",
    "\n",
    "1. Users have to learn an entirely new framework to work with distributed computing problems\n",
    "2. Logic written for a *small data* project is not reusable for a *big data* project\n",
    "3. Testing becomes a heavyweight process for distributed computing, especially Spark\n",
    "4. Along with number 3, iterations for distributed computing problems become slower and more expensive\n",
    "\n",
    "Fugue's core principle is to minimize code dependency on frameworks as much as possible, which provides flexibility and portability. **By decoupling logic and execution, we can focus on our logic in a scale-agnostic way and then choose which execution engine to use when the time arises.** In this section, we look at how to move from the `transform()` function to end-to-end workflows with `FugueWorkflow()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences Between Pandas and Spark\n",
    "\n",
    "To illustrate the first two points above, we'll use the same example that we used in the Type Flexibility section. We want to create a new column called `location` using the mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(217)-123-4567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(217)-234-5678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(407)-123-4567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(407)-234-5678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(510)-123-4567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            phone\n",
       "0  (217)-123-4567\n",
       "1  (217)-234-5678\n",
       "2  (407)-123-4567\n",
       "3  (407)-234-5678\n",
       "4  (510)-123-4567"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "_area_code_map = {\"217\": \"Champaign, IL\", \"407\": \"Orlando, FL\", \"510\": \"Fremont, CA\"}\n",
    "\n",
    "data = pd.DataFrame({\"phone\": [\"(217)-123-4567\", \"(217)-234-5678\", \"(407)-123-4567\", \n",
    "                               \"(407)-234-5678\", \"(510)-123-4567\"]})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll perform the operation in Pandas. It's very simple because of the `.map()` method in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(217)-123-4567</td>\n",
       "      <td>Champaign, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(217)-234-5678</td>\n",
       "      <td>Champaign, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(407)-123-4567</td>\n",
       "      <td>Orlando, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(407)-234-5678</td>\n",
       "      <td>Orlando, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(510)-123-4567</td>\n",
       "      <td>Fremont, CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            phone       location\n",
       "0  (217)-123-4567  Champaign, IL\n",
       "1  (217)-234-5678  Champaign, IL\n",
       "2  (407)-123-4567    Orlando, FL\n",
       "3  (407)-234-5678    Orlando, FL\n",
       "4  (510)-123-4567    Fremont, CA"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_phone_to_location(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"location\"] = df[\"phone\"].str.slice(1,4).map(_area_code_map)\n",
    "    return df\n",
    "\n",
    "map_phone_to_location(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll perform the same operation in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Spark session\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|         phone|     location|\n",
      "+--------------+-------------+\n",
      "|(217)-123-4567|Champaign, IL|\n",
      "|(217)-234-5678|Champaign, IL|\n",
      "|(407)-123-4567|  Orlando, FL|\n",
      "|(407)-234-5678|  Orlando, FL|\n",
      "|(510)-123-4567|  Fremont, CA|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, col, lit, substring\n",
    "from itertools import chain\n",
    "\n",
    "df = spark.createDataFrame(data)  # converting the previous Pandas DataFrame\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*_area_code_map.items())])\n",
    "\n",
    "def map_phone_to_location(df: DataFrame) -> DataFrame:\n",
    "    _df = df.withColumn(\"location\", mapping_expr[substring(col(\"phone\"),2,3)])\n",
    "    return _df\n",
    "\n",
    "map_phone_to_location(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the two code examples, we had to re-implement the exact same functionality with completely different syntax. This isn't a cherry-picked example. Data practitioners will often have to write two implementations of the same logic, one for each framework, especially as the logic gets more specific. \n",
    "\n",
    "This is where Fugue comes in. Users can use the abstraction layer to write one implementation of the function. This can then be applied to Pandas, Spark, and Dask. We already saw this with the `transform()` function. An example snippet can be found below, nothing will be new here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(217)-123-4567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(217)-234-5678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(407)-123-4567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(407)-234-5678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(510)-123-4567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            phone\n",
       "0  (217)-123-4567\n",
       "1  (217)-234-5678\n",
       "2  (407)-123-4567\n",
       "3  (407)-234-5678\n",
       "4  (510)-123-4567"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugue import transform\n",
    "\n",
    "def map_phone_to_location(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"location\"] = df[\"phone\"].str.slice(1,4).map(_area_code_map)\n",
    "    return df\n",
    "\n",
    "transform(data.copy(),\n",
    "          map_phone_to_location,\n",
    "          schema=\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `transform` versus `FugueWorkflow`\n",
    "\n",
    "While the `transform()` function is good for running a single function across multiple execution engines, Fugue also has `FugueWorkflow`, which can be used to make engine-agnostic end-to-end workflows. `FugueWorkflow()` constructs a directed-acyclic graph (DAG) where the inputs and outputs are DataFrames. The code block below will run on the Pandas-based `NativeExecutionEngine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "phone:str                                                      |location:str             \n",
      "---------------------------------------------------------------+-------------------------\n",
      "(217)-123-4567                                                 |Champaign, IL            \n",
      "(217)-234-5678                                                 |Champaign, IL            \n",
      "(407)-123-4567                                                 |Orlando, FL              \n",
      "(407)-234-5678                                                 |Orlando, FL              \n",
      "(510)-123-4567                                                 |Fremont, CA              \n",
      "Total count: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data.copy())\n",
    "    df = df.transform(map_phone_to_location, schema=\"*, location:str\")\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bring it to Spark, all we need to do is pass the `SparkExecutionEngine` into `FugueWorkflow`, similar to how we used the `transform()` function in the last section. All the code underneath the `with` statement will run on Spark. We did not make any modifications to `map_phone_to_location` to bring it to Spark. The `df.transform()` call below converts it to a Fugue `Transformer` during runtime by using the type-annotations and schema provided. We can use the same function in Spark or Dask without making modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkDataFrame\n",
      "phone:str                                                      |location:str             \n",
      "---------------------------------------------------------------+-------------------------\n",
      "(217)-123-4567                                                 |Champaign, IL            \n",
      "(217)-234-5678                                                 |Champaign, IL            \n",
      "(407)-123-4567                                                 |Orlando, FL              \n",
      "(407)-234-5678                                                 |Orlando, FL              \n",
      "(510)-123-4567                                                 |Fremont, CA              \n",
      "Total count: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with FugueWorkflow(spark) as dag:\n",
    "    df = dag.df(data.copy())  # Still the original Pandas DataFrame\n",
    "    df = df.transform(map_phone_to_location, schema=\"*, location:str\")\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had five different functions that we call `transform()` on to bring to Spark, we would need to specify the `SparkExecutionEngine` five times. The `FugueWorkflow` allows us to make the entire computation run on either Pandas, Spark, or Dask. Both are similar in principle in that they leave the original functions decoupled from the execution environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testability and Maintainability\n",
    "\n",
    "Is the native Python or Pandas implementation of `map_phone_to_location()` better or is the native Spark implementation better? \n",
    "\n",
    "The main concern of Fugue is clear, readable code. **Users can write code in whatever expresses their logic the best**. The computing efficiency lost by using Fugue is unlikely to be significant, especially in comparison to the developer efficiency gained through more rapid iterations and easier maintenance. In fact, Fugue is designed in a way that often sees more speed-ups than inexperienced users working with native Spark code because it handles a lot of the tricks necessary to use Spark effectively. \n",
    "\n",
    "Fugue code becomes easily testable because the function contains logic that is portable across all Pandas, Spark, and Dask. We can test code without the need to spin up computing resources (such as Spark or Dask clusters). This hardware often takes time to spin up just for a simple test, making it painful to run unit tests on Spark. Now, we can test quickly with native Python or Pandas and then execute on Spark when needed. Developers that use Fugue benefit from more rapid iterations in their data projects.\n",
    "\n",
    "If we use a pure Python function, such as the one below, all we have to do to test it is run some values through the defined function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'phone': '(407)-234-5678', 'location': 'Orlando, FL'},\n",
       " {'phone': '(407)-234-5679', 'location': 'Orlando, FL'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def map_phone_to_location2(df: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n",
    "    for row in df:\n",
    "        row[\"location\"] = _area_code_map[row[\"phone\"][1:4]]\n",
    "    return df\n",
    "\n",
    "# Remember the input was List[Dict[str,Any]]\n",
    "map_phone_to_location2([{'phone': '(407)-234-5678'}, \n",
    "                       {'phone': '(407)-234-5679'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the output here is a `List[Dict[str,Any]]`, Fugue takes care of converting it back to a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fugue as a Mindset\n",
    "\n",
    "Fugue is a framework, but more importantly, it is a mindset. \n",
    "\n",
    "1. Fugue believes that the framework should adapt to the user, not the other way around.\n",
    "2. Fugue lets users code express logic in a scale-agnostic way, with the tools they prefer.\n",
    "3. Fugue values readability and maintainability of code over deep framework-specific optimizations\n",
    "\n",
    "Using distributed computing is currently harder than it needs to be. However, these systems often follow similar patterns, which have been abstracted to create a framework that lets users focus on defining their logic. We cover these concepts in the rest of tutorials. If you're new to distributed computing, Fugue is the perfect place to get started.\n",
    "\n",
    "## [Optional] Comparison to Modin and Koalas\n",
    "\n",
    "Fugue gets compared a lot to Modin and Koalas. Modin is a Pandas interface for execution on Dask, and Koalas is a Pandas interface for execution on Spark. Fugue, Modin, and Koalas have similar goals in making an easier distributed computing experience. The main difference is that Modin and Koalas use Pandas as the grammar for distributed computing. Fugue, on the other hand, uses native Python and SQL as the grammar for distributed computing (though Pandas is also supported). For more information, check this [page](https://fugue-tutorials.readthedocs.io/tutorials/appendix/fugue_not_pandas.html).\n",
    "\n",
    "The clearest example of Pandas not being compatible with Spark is the acceptance of mixed-typed columns. A single column can have numeric and string values. Spark, on the other hand, is strongly typed and enforces the schema. More than that, Pandas is strongly reliant on the index for operations. As users transition to Spark, the index mindset does not hold as well. Order is not always guaranteed in a distributed system; there is an overhead to maintain a global index, and, moreover, it is often not necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "f7f9294720e464cd08733c6cd5cfe1a4599977fa03668bc63f2dfd97f1a61807"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
