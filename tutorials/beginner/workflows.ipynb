{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows\n",
    "\n",
    "Have questions? Chat with us on Github or Slack:\n",
    "\n",
    "[![Homepage](https://img.shields.io/badge/fugue-source--code-red?logo=github)](https://github.com/fugue-project/fugue)\n",
    "[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://slack.fugue.ai)\n",
    "\n",
    "In the previous section, we talked about the motivation of Fugue and showed a simple example of `FugueWorkflow`. Here we provide more details about the Fugue syntax that will be needed in actual applications. We will also talk about some of the underlying concepts that were shown in the previous section but not really explained.\n",
    "\n",
    "## The Directed Acyclic Graph (DAG)\n",
    "\n",
    "In the last section, we had a code block that used `with FugueWorkflow() as dag:`. We'll demonstrate another simple example below. Here we have a simple `transformer` that takes in two columns and adds them to create a third column called `col3`. This is then used in the `FugueWorkflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "col1:long|col2:long|col3:int\n",
      "---------+---------+--------\n",
      "1        |2        |3       \n",
      "2        |3        |5       \n",
      "3        |4        |7       \n",
      "Total count: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "\n",
    "data = pd.DataFrame({'col1': [1,2,3], 'col2':[2,3,4]})\n",
    "data2 = data.copy()\n",
    "\n",
    "def make_new_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['col3'] = df['col1'] + df['col2']\n",
    "    return df \n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data2)\n",
    "    df = df.transform(make_new_col, schema=\"*, col3:int\")\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing here is new. This should all be familiar from the last section; however, we did not really dive into what the `FugueWorkflow` actually does. The `FugueWorkflow` is responsible for constructing a Directed Acyclic Graph, also called a DAG. A lot of people associate the DAG concept with workflow orchestration tools like Airflow, Prefect, or Dagster. While these tools also use DAGs, they use it in a different way than the distributed computing frameworks (Spark and Dask). For orchestration frameworks, the DAG is used to manage dependencies of scheduled tasks. For computing frameworks, the DAG represents a computation graph that is built, validated, and then executed. DAGs are used because distributed computing operations are very expensive and have a lot of room to be optimized. Also, mistakes in a distributed setting are very expensive.\n",
    "\n",
    "Fugue follows these distributed computing frameworks in using the DAG for validation before execution. DAGs can catch errors significantly earlier, in a way similar to compiling the computing job. For Fugue specifically, the built DAG validates schema, as well as provides the basis for further optimizations. For example, Fugue can detect which DataFrames are re-used in the computation graph and then persist them automatically to avoid recomputation. The DAG is a graph where the nodes are DataFrames connected by Fugue extensions. We already introduced the most common extension, which is the `transformer`. Schema is tracked throughout the DAG. More extensions will be introduced later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Parameters\n",
    "\n",
    "We saw in the first section how to pass parameters into the `transform()` function but we haven't seen how to do this yet with `FugueWorkflow`. Passing parameters is identical in both approaches. We pass them as a dictionary when we use the `transform()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "col1:long|col2:long|col3:int\n",
      "---------+---------+--------\n",
      "1        |2        |13      \n",
      "2        |3        |15      \n",
      "3        |4        |17      \n",
      "Total count: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = data.copy()\n",
    "\n",
    "# schema: *, col3:int\n",
    "def make_new_col(df: pd.DataFrame, n=1) -> pd.DataFrame:\n",
    "    df['col3'] = df['col1'] + df['col2'] + n\n",
    "    return df \n",
    "    \n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data2)\n",
    "    df = df.transform(make_new_col, params={'n': 10})  # Pass parameters\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Saving Data\n",
    "\n",
    "Load and save operations are done inside the `FugueWorkflow` and use the appropriate saver/loader for the file extension (.csv, .json, .parquet, .avro) and ExecutionEngine (Pandas, Spark, or Dask). For distributed computing, parquet and avro tend to be the most used due to compression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "col1:int|col2:int\n",
      "--------+--------\n",
      "1       |2       \n",
      "2       |3       \n",
      "3       |4       \n",
      "Total count: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(data2)\n",
    "    df.save('/tmp/data.parquet', mode=\"overwrite\", single=True)\n",
    "    df.save(\"/tmp/data.csv\", mode=\"overwrite\", header=True)\n",
    "    df2 = dag.load('/tmp/data.parquet')\n",
    "    df3 = dag.load(\"/tmp/data.csv\", header=True, columns=\"col1:int,col2:int\")\n",
    "    df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section we covered some concepts, such as the DAG, and why explicit schema is needed. We also covered how to define schema and pass in parameters. Combined with loading and saving of files, users can already start using Fugue for working with data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
